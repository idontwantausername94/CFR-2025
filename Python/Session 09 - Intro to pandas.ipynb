{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 9: Introduction to Pandas and DataFrames\n",
    "\n",
    "## Introduction\n",
    "Pandas is a powerful data manipulation library in Python, providing data structures and functions needed to manipulate structured data seamlessly. In this tutorial, you will learn about Pandas and its core data structure, the DataFrame.\n",
    "\n",
    "## Objectives\n",
    "- Understand the basics of Pandas\n",
    "- Learn how to create and manipulate DataFrames\n",
    "- Perform data analysis with Pandas\n",
    "\n",
    "## Prerequisites\n",
    "- Knowledge of Python: functions, arrays, lists, NumPy, loops, conditionals\n",
    "- Basic understanding of data visualization with Matplotlib\n",
    "\n",
    "## Estimated Time\n",
    "1.5 hours\n",
    "\n",
    "## Part 1: Introduction to Pandas (20 minutes)\n",
    "\n",
    "**What is Pandas?**\n",
    "Pandas is an open-source library providing high-performance, easy-to-use data structures, and data analysis tools for Python. It is particularly useful for data manipulation and analysis.\n",
    "\n",
    "**Installing Pandas (should already be installed)**\n",
    "If you haven't installed Pandas yet, you can do so using pip: pip install pandas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Importing Pandas**\n",
    "```python\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating DataFrames**\n",
    "\n",
    "#### Example 1: Creating a DataFrame from an Excel file\n",
    "\n",
    "**Instructions:**\n",
    "1. Create an Excel file with some sample data and save as a .csv file\n",
    "2. Load the file as a DataFrame using `pd.read_csv()`\n",
    "\n",
    "**Specific Instructions:***\n",
    "We have information about the following '90s rappers:\n",
    "\n",
    "- __Tupac__ was born in __Manhattan__ and signed his first record deal at age __20__.  \n",
    "- __The Notorious B.I.G.__ was born in __Brooklyn__ and signed his first deal at age __19__.  \n",
    "- __Snoop Dogg__ was born in __Long Beach__ and signed his first deal at age __19__.  \n",
    "- __Nas__ was born in __Brooklyn__ and signed his first deal at age __18__.  \n",
    "- __Dr. Dre__ was born in __Compton__ and signed his first deal at age __19__.\n",
    "\n",
    "Together, let's create an Excel file with columns \"Name\", \"City\", and \"Age\", and save it as \"rappers.csv\" (File > Save As > File Format: CSV UTF-8).\n",
    "We will upload this file to Jupyter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'rappers.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m rappers_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrappers.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(rappers_data)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'rappers.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "rappers_data = pd.read_csv('rappers.csv')\n",
    "print(rappers_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice Problem 1: Create Your Own DataFrame\n",
    "\n",
    "Create a DataFrame for the following data:\n",
    "\n",
    "| Product    | Price | Quantity |\n",
    "|------------|-------|----------|\n",
    "| Laptop     | 1000  | 50       |\n",
    "| Tablet     | 500   | 30       |\n",
    "| Smartphone | 800   | 100      |  \n",
    "\n",
    "Use Excel to enter the data, save it as a CSV file, upload it to Jupyter, then read it with pandas. Name the variable \"df\".\n",
    "\n",
    "**Solution:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2: Data Manipulation with Pandas (30 minutes)\n",
    "\n",
    "### Selecting Data\n",
    "\n",
    "#### Example 2: Selecting Columns and Rows\n",
    "\n",
    "Instructions:\n",
    "- Select a single column using `df['column_name']`.\n",
    "- Select multiple columns using `df[['column1', 'column2']]`.\n",
    "- Select rows using `df.iloc[]` for integer-location based indexing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting a single column\n",
    "products = df['Product']\n",
    "print(products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Selecting multiple columns\n",
    "product_price = df[['Product', 'Price']]\n",
    "print(product_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting rows by index\n",
    "first_row = df.iloc[0]\n",
    "print(first_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting a subset of rows and columns\n",
    "subset = df.iloc[0:2, 0:3]\n",
    "print(subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice Problem 2: Select Data from Your DataFrame\n",
    "\n",
    "1. Select the **'Product'** and **'Price'** columns from your DataFrame.  \n",
    "2. Next, select the first two rows from your DataFrame.  \n",
    "3. Finally, select the first two rows and two columns.\n",
    "\n",
    "**Solution:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering Data\n",
    "\n",
    "**Example 3: Filtering Data Based on Conditions**\n",
    "\n",
    "Instructions:\n",
    "- Filter rows where a column meets a condition using boolean indexing.\n",
    "- Combine multiple conditions using & (and) and | (or).  \n",
    "- **Python evaluates & (and) before | (or).**.  \n",
    "    For example:\n",
    "    <pre markdown>\n",
    "    True or False and True\n",
    "    # evaluates to \"True or (False and True)\"\n",
    "    # which equals \"True or False\"\n",
    "    # which equals True\n",
    "    </pre>\n",
    "- Use parentheses () to override this order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where 'Price' is greater than 600\n",
    "filtered_data = df[df['Price'] > 600]\n",
    "print(filtered_data)\n",
    "\n",
    "# Combine multiple conditions using & (and) and | (or)\n",
    "filtered_data = df[(df['Price'] > 600) & (df['Quantity'] < 80)]\n",
    "print(filtered_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice Problem 3: Filter Data in Your DataFrame\n",
    "\n",
    "(a) Filter rows where the 'Price' is greater than 600 and 'Quantity' is less than 100.  \n",
    "(b) Filter rows where either the 'Price' is less than 600, or 'Quantity is greater than 60.  \n",
    "(c) Filter rows where the 'Price' is greater than 600 and the 'Quantity' is either greater than 80 or less than 60.\n",
    "\n",
    "**Solution:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding and Removing Data\n",
    "\n",
    "#### Example 4: Adding and Removing Columns\n",
    "\n",
    "Instructions:\n",
    "Add a new column to the DataFrame.\n",
    "Remove a column using df.drop().\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a new column\n",
    "df['Discount'] = [0.1, 0.2, 0.15]\n",
    "\n",
    "# Removing a column\n",
    "df.drop('Discount', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice Problem 4: Add and Remove Columns in Your DataFrame\n",
    "\n",
    "Add a 'Discount' column to your DataFrame with values [10, 15, 20]. Then, remove the 'Quantity' column.  \n",
    "**Solution:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Interested Students: Advanced DataFrame Operations\n",
    "\n",
    "**Handling Missing Data**\n",
    "\n",
    "#### Example 5: Identifying and Handling Missing Data*\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "- Identify missing data using `df.isnull()`.\n",
    "- Drop rows with missing data using `df.dropna()`.\n",
    "- Fill missing data using `df.fillna()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# load the rappers data for this example\n",
    "df = pd.read_csv('rappers.csv')\n",
    "\n",
    "# Adding missing data\n",
    "df.loc[2, 'Age'] = np.nan\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying missing data\n",
    "print(df.isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping rows with missing data\n",
    "df_dropped = df.dropna()\n",
    "print(df_dropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling missing data\n",
    "df_filled = df.fillna(0)\n",
    "print(df_filled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice Problem 5: Handle Missing Data in Your DataFrame\n",
    "\n",
    "(a) Set the price of the first item to missing (np.nan).  \n",
    "(b) Then create a copy of your dataframe, but drop all rows with missing data.  \n",
    "(c) Finally, create a copy of your dataframe, but fill all missing data with 0.  \n",
    "\n",
    "Note: to set the price of the _n_-th item, use: df.loc[_n_, 'Price'] = ...\n",
    "\n",
    "**Solution:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping and Aggregating Data\n",
    "\n",
    "**Example 6: Grouping and Aggregating Data**\n",
    "\n",
    "**Instructions:**\n",
    "- Group data using `df.groupby()`.\n",
    "- Perform aggregation functions such as `sum()`, `mean()`, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('rappers.csv')\n",
    "\n",
    "# Group and aggregate\n",
    "grouped = df.groupby('Artist')['NetWorth']\n",
    "sum_networth = grouped.sum()\n",
    "mean_networth = grouped.mean()\n",
    "\n",
    "print(\"Sum of all Estimates by Artist:\")\n",
    "print(sum_networth)\n",
    "print(\"\\nAverage Net Worth Estimate by Artist:\")\n",
    "print(mean_networth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice Problem 6: Group and Aggregate Data in Your DataFrame\n",
    "\n",
    "You now have the following data:  \n",
    "| Product    | Price | Quantity |\n",
    "|------------|-------|----------|\n",
    "| Laptop     | 1000  | 50       |\n",
    "| Tablet     | 500   | 30       |\n",
    "| Smartphone | 800   | 100      |\n",
    "| Laptop     | 1200  | 70       |\n",
    "| Tablet     | 450   | 20       |\n",
    "\n",
    "Create a dataframe from the above data, group by 'Product' and calculate the total and average price for each product.\n",
    "\n",
    "**Solution:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging and Joining DataFrames\n",
    "\n",
    "#### Example 8: Merging DataFrames\n",
    "\n",
    "Instructions:  \n",
    "- Create dataframes from Table 1 and Table 2, named \"table1.csv\" and \"table2.csv\", respectively.  \n",
    "- Merge two DataFrames using pd.merge().\n",
    "- Perform different types of merges: inner, outer, left, right.  \n",
    "\n",
    "    - _inner_: keep **only rows whose ID appears in both tables**.\n",
    "    - _outer_: keep **every row** from both tables, **filling gaps with NaN.**\n",
    "    - _left_: keep **all rows from the first table** and **only matching rows from the second**.  \n",
    "    - right: keep **all rows from the second table** and **only matching rows from the first**.  \n",
    "      \n",
    "Table 1:  \n",
    "| ID | Artist     | Record Label             |\n",
    "|----|------------|--------------------------|\n",
    "| 1  | Tupac      | Interscope               |\n",
    "| 2  | Biggie     | Bad Boy Records          |\n",
    "| 3  | Snoop Dogg | Doggystyle Records       |\n",
    "| 4  | Dr. Dre    | Aftermath Entertainment  |  \n",
    "\n",
    "Table 2:  \n",
    "| ID | Age |\n",
    "|----|-----|\n",
    "| 3  | 23  |\n",
    "| 4  | 31  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv(\"table1.csv\")\n",
    "df2 = pd.read_csv(\"table2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inner join\n",
    "merged_df_inner = pd.merge(df1, df2, on=\"ID\", how=\"inner\")\n",
    "print(merged_df_inner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outer join\n",
    "merged_df_outer = pd.merge(df1, df2, on=\"ID\", how=\"outer\")\n",
    "print(merged_df_outer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# left join\n",
    "merged_df_left = pd.merge(df1, df2, on=\"ID\", how=\"left\")\n",
    "print(merged_df_left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# right join\n",
    "merged_df_right = pd.merge(df1, df2, on=\"ID\", how=\"right\")\n",
    "print(merged_df_right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice Problem 8: Merge DataFrames\n",
    "\n",
    "Create two DataFrames with some common columns and merge them using an inner join.\n",
    "\n",
    "**Solution:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading from a data file\n",
    "\n",
    "#### Example 9: Load a CSV file\n",
    "\n",
    "Instructions:\n",
    "- use pd.read_CSV(file_name) to convert the Berkeley Earth data to pandas\n",
    "- you can now use everything we learned above to analyze it!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example CSV\n",
    "file_name = 'GAST_BerkeleyEarth_1850-2023.csv'\n",
    "berkeley_earth_df = pd.read_csv(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice Problem 9: Analyze a data file\n",
    "\n",
    "(a) Read the Berkeley Earth data from above, group by the 'Year' column, and compute the average temperature per year (temperature is in the 'Monthly Average' column).\n",
    "(b) Make a plot of average temperature versus year.  \n",
    "\n",
    "Hint 1: group by 'Year', select the 'Monthly Average' column, take the mean, then use the .reset_index() method.  \n",
    "Example: <pre markdown> grouped = df.groupby(COL_X)[COL_Y].mean().reset_index() </pre>\n",
    "  \n",
    "Hint 2: You can loop through the rows of a dataframe using the following syntax:  \n",
    "<pre markdown>\n",
    "for index, row in df.iterrows():\n",
    "    year = index\n",
    "    avg_temp = row['Monthly Average']\n",
    "    â€¦\n",
    "</pre>\n",
    "\n",
    "**Solution:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, you have learned how to:\n",
    "- Create and manipulate DataFrames using Pandas\n",
    "- Select, filter, add, and remove data\n",
    "- Handle missing data\n",
    "- Group and aggregate data\n",
    "- Merge and join DataFrames\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
